{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## AI1104: Programming with AI\n",
    "\n",
    "Homework 2: Backpropagation \n",
    "\n",
    "Author: Tanmay Goyal, AI20BTECH11021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importing all required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def samples(N , network_type):\n",
    "    '''\n",
    "    generates training and testing samples, as well as training and testing labels\n",
    "    \n",
    "    param:\n",
    "    N = total number of samples being used\n",
    "    network_type = the binary operation our MLP would be learning, can be XOR, AND or OR\n",
    "\n",
    "    returns:\n",
    "    X_train = set of all training points\n",
    "    X_test = set of all testing points\n",
    "    y_train = set of all labels cooresponding to the training points\n",
    "    y_test = set of all labels corresponding to the testing points\n",
    "\n",
    "    '''\n",
    "\n",
    "    # generating the covariance matrix\n",
    "    cov = 0.05 * np.identity(2)\n",
    "    \n",
    "    # first quarter of training points would be centered around [0,0]\n",
    "    mean1 = [0,0]\n",
    "    x1 = np.random.multivariate_normal(mean1 , cov , N//4)\n",
    "    \n",
    "    # second quarter of training points would be centered around [0,1]\n",
    "    mean2 = [0,1]\n",
    "    x2 = np.random.multivariate_normal(mean2 , cov , N//4)\n",
    "\n",
    "    # third quarter of training points would be centered around [1,0]\n",
    "    mean3 = [1, 0]\n",
    "    x3 = np.random.multivariate_normal(mean3 , cov , N//4)\n",
    "\n",
    "    # fourth quarter of training points would be centered around [1,1]\n",
    "    mean4 = [1,1]\n",
    "    x4 = np.random.multivariate_normal(mean4 , cov , N//4)\n",
    "    \n",
    "    # collecting all these points together\n",
    "    X = np.vstack((x1 , x2 , x3 , x4))\n",
    "    \n",
    "    # we would generate 0.8N random indices to choose our training samples\n",
    "    training_samples_indices = np.random.choice([i for i in range(N)] , size = int(0.8*N) , \\\n",
    "                                                replace = False)\n",
    "    \n",
    "    # the indices not selected for training would be used for testing\n",
    "    testing_samples_indices = np.array([i for i in range(N) if i not in training_samples_indices])\n",
    "\n",
    "    # initialising X_train and X_test with a row of (0,0) which would later be deleted\n",
    "    X_train = np.array([[0,0]])\n",
    "    X_test = np.array([[0,0]])\n",
    "\n",
    "    # We now partition all our points into training and testing sets, based on the indices\n",
    "    for i in training_samples_indices:\n",
    "       X_train = np.vstack((X_train , X[i][:]))\n",
    "\n",
    "    for i in testing_samples_indices:\n",
    "        X_test = np.vstack((X_test , X[i][:]))\n",
    "\n",
    "    # we delete the first row of (0,0) we had used to initialise X_train and X_test\n",
    "    X_train = np.delete(X_train , 0 , 0)\n",
    "    X_test = np.delete(X_test , 0 , 0)\n",
    "\n",
    "    # finally we add a column of ones alongside the datapoints\n",
    "    X_train = np.hstack((np.ones((X_train.shape[0],1)) , X_train))\n",
    "    X_test = np.hstack((np.ones((X_test.shape[0],1)) , X_test))  \n",
    "\n",
    "    # based on the network type, we now prepare our set of labels, in the same sequence of \n",
    "    # means used to prepare our initial set of points\n",
    "\n",
    "    if(network_type == \"XOR\"):\n",
    "        y1 = np.array([0 for i in range(N//4)])  # 0 XOR 0 = 0\n",
    "        y2 = np.array([1 for i in range (N//4)]) # 0 XOR 1 = 1\n",
    "        y3 = np.array([1 for i in range (N//4)]) # 1 XOR 0 = 1\n",
    "        y4 = np.array([0 for i in range(N//4)])  # 1 XOR 1 = 0\n",
    "        Y = np.vstack((y1 , y2 , y3 , y4))\n",
    "    \n",
    "    elif(network_type == \"AND\"):\n",
    "        y1 = np.array([0 for i in range(N//4)])  # 0 AND 0 = 0\n",
    "        y2 = np.array([0 for i in range (N//4)]) # 0 AND 1 = 0\n",
    "        y3 = np.array([0 for i in range (N//4)]) # 1 AND 0 = 0\n",
    "        y4 = np.array([1 for i in range(N//4)])  # 1 AND 1 = 1\n",
    "        Y = np.vstack((y1 , y2 , y3 , y4))\n",
    "\n",
    "    elif(network_type == \"OR\"):\n",
    "        y1 = np.array([0 for i in range(N//4)])  # 0 OR 0 = 0\n",
    "        y2 = np.array([1 for i in range (N//4)]) # 0 OR 1 = 1\n",
    "        y3 = np.array([1 for i in range (N//4)]) # 1 OR 0 = 1\n",
    "        y4 = np.array([1 for i in range(N//4)])  # 1 OR 1 = 1\n",
    "        Y = np.vstack((y1 , y2 , y3 , y4))\n",
    "\n",
    "    # reshaping Y to N rows and 1 column to match the shape of X\n",
    "    Y = Y.reshape((N,1))\n",
    "    \n",
    "    # initialising y_train and y_test with a row of zeroes which would later be deleted\n",
    "    y_train = np.array([0])\n",
    "    y_test = np.array([0])\n",
    "    \n",
    "    # We now partition all our points into training and testing labels, based on the indices\n",
    "    for i in training_samples_indices:\n",
    "       y_train = np.vstack((y_train , Y[i][:]))\n",
    "\n",
    "    for i in testing_samples_indices:\n",
    "        y_test = np.vstack((y_test , Y[i][:]))\n",
    "\n",
    "    # we finally delete the first row of (0,0) we had used to initialise y_train and y_test\n",
    "    y_train = np.delete(y_train , 0 , 0)\n",
    "    y_test = np.delete(y_test , 0 , 0)\n",
    "\n",
    "    return X_train, X_test, y_train , y_test\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The non- linearity function we are using is the sigmoid, given by:\n",
    "\n",
    "$\\sigma (x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "The derivative of sigmoid results in an interesting finding:\n",
    "\n",
    "$\\sigma '(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\sigma(x) \\times (1 - \\sigma(x))$\n",
    "\n",
    "Since, $\\sigma(x)$ at any node is equal to $output$, we can write:\n",
    "\n",
    "$\\sigma '(x) = output \\times (1-output)$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# we use the above definitions of sigmoid to define our function\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The update rule is as follows:\n",
    "\n",
    "$w_{ij}'= w_{ij} - \\gamma \\frac{\\delta R(\\theta)}{\\delta w_{ij}}$\n",
    "\n",
    "\n",
    "We will use the following parameters:\n",
    "\n",
    "Number of epochs = 100\n",
    "\n",
    "Learning rate = $\\gamma = 0.05$\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def backpropagate_and_report(N , network_type):\n",
    "    '''\n",
    "    This function will train the model, and update the weights using backpropagation.\n",
    "    It will also report the training and testing error by plotting the same.\n",
    "\n",
    "    param:\n",
    "    N = total number of samples being used\n",
    "    network_type = the binary operation our MLP would be learning, can be XOR, AND or OR\n",
    "\n",
    "    returns:\n",
    "    None\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # we would first generate the samples\n",
    "    X_train , X_test , y_train , y_test = samples(N , network_type)\n",
    "\n",
    "    # the number of epochs is equal to 100, predefined\n",
    "    epochs = 100\n",
    "\n",
    "    # the learning rate = 0.05, predefined\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    # we want 20 updates per epoch, we donot want to fix the batch size because\n",
    "    # then as the number of samples increase, the number of updates increase\n",
    "    # i.e for lesser number of samples, updates would be lesser\n",
    "    batch_size = len(X_train)//20\n",
    "    \n",
    "    # we make two empty lists for storing training and testing errors\n",
    "    training_error_list = []\n",
    "    testing_error_list = []\n",
    "\n",
    "    # we would randomly initialise the weights between -1 and 1\n",
    "    # since the datapoint is initially of the form [1 x1 x2]\n",
    "    # the weights between hidden layer and input layer would look like [w0 w1 w2].T\n",
    "    # for two hidden layers, we would end up with a set of weights with shape (3,2)\n",
    "    # i.e 6 elements\n",
    "    # for initialisation in (a,b), we use [a + (b-a) * np.random.rand()]\n",
    "    weights_hidden_input = np.array([2 * np.random.rand() - 1 for i in range(6)]).reshape((3,2))\n",
    "   \n",
    "    # similarly the output from the hidden layer will be of the form [1 z1 z2]\n",
    "    # so the weights would be of the form [w0 w1 w2].T\n",
    "    # for one output layer, we would end up with a set of weights with shape (3,1)\n",
    "    # i.e 3 elements\n",
    "    weights_output_hidden = np.array([2 * np.random.rand() - 1 for i in range(3)]).reshape((3,1))\n",
    "    \n",
    "    \n",
    "    # with everything set up, we can finally begin training our model \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # setting up our index to keep track of the batches\n",
    "        for idx in range(batch_size , len(X_train) , batch_size):\n",
    "           \n",
    "            # we initialise weight_update matrices\n",
    "            weights_update_hidden_input = np.zeros(weights_hidden_input.shape)\n",
    "            weights_update_output_hidden = np.zeros(weights_output_hidden.shape)\n",
    "            \n",
    "            #setting up i for all the indices within that batch\n",
    "            for i in range(idx - batch_size , idx):\n",
    "                \n",
    "                # we would pull out the data point from the training set\n",
    "                x = X_train[i][:].reshape(1,3)\n",
    "                \n",
    "                # output from the hidden layers is equal to sigmoid(x * W)\n",
    "                # we have already made sure the dimensions are compatible with one another \n",
    "                z = sigmoid(np.matmul(x , weights_hidden_input))\n",
    "            \n",
    "                # we insert a 1 before the z to account for the bias from the next set of weights\n",
    "                z_with_1 = np.hstack((np.ones((1,1)) , z))\n",
    "                   \n",
    "                # the final output will be sigmoid(z * w)\n",
    "                y_hat = sigmoid(np.matmul(z_with_1 , weights_output_hidden))\n",
    "                    \n",
    "                # error term = -2 * (y - y_hat) * sigmoid_prime\n",
    "                # error term = -2 * (y - y_hat) * y_hat * (1 - y_hat)\n",
    "                error_term = -2 * (y_train[i] - y_hat) * y_hat * (1-y_hat)\n",
    "\n",
    "                # each weight between output and hidden layer is updated by \n",
    "                # learning_rate * error_term * corresponding input\n",
    "                # since inputs were [1 z1 z2], we take transpose to convert them into the shape of the weights\n",
    "                weights_update_output_hidden += (learning_rate * error_term * z_with_1.T)\n",
    "                \n",
    "                #we want the weights between ouput layer and hidden layer without the intercept term\n",
    "                weights_without_bias = weights_output_hidden[1:]\n",
    "\n",
    "                # each weight between hidden layer and input layer would be updated as follows:\n",
    "                # eg. weight between first input and first hidden layer = \n",
    "                # learning rate * error term * first weight between output and hidden * z1_prime * x1\n",
    "                # =  learning rate * error term * first weight between output and hidden * z1 * (1-z1) * x1\n",
    "                weights_update_hidden_input += (learning_rate * error_term * weights_without_bias * z.T * (1-z.T) * x).T\n",
    "                \n",
    "            \n",
    "            # after one batch is completed, we update the weights\n",
    "            weights_output_hidden -= weights_update_output_hidden\n",
    "            weights_hidden_input -= weights_update_hidden_input\n",
    "\n",
    "\n",
    "        # we are done with all batches and one epoch is done, \n",
    "        # so now we calculate training error and testing error:\n",
    "        training_error = 0\n",
    "        testing_error = 0\n",
    "        \n",
    "        #calculating training error\n",
    "        for idx in range(len(X_train)):\n",
    "                    \n",
    "            # we would pull out the data point from the training set    \n",
    "            x = X_train[idx][:].reshape(1,3)\n",
    "                    \n",
    "            # output from the hidden layers is equal to sigmoid(x * W)\n",
    "            # we have already made sure the dimensions are compatible with one another \n",
    "            z = sigmoid(np.matmul(x , weights_hidden_input))\n",
    "            \n",
    "            # we insert a 1 before the z to account for the bias from the next set of weights\n",
    "            z_with_1 = np.hstack((np.ones((1,1)) , z))\n",
    "                    \n",
    "            # the final output will be sigmoid(z * w)\n",
    "            y_hat = sigmoid(np.matmul(z_with_1 , weights_output_hidden))\n",
    "            \n",
    "            # error = (y-y_hat)^2\n",
    "            training_error += (y_train[idx] - y_hat) ** 2\n",
    "\n",
    "        #averaging the error\n",
    "        training_error /= len(X_train)\n",
    "\n",
    "        # calculating the testing error\n",
    "        for idx in range(len(X_test)):\n",
    "            \n",
    "            # we would pull out the data point from the training set        \n",
    "            x = X_test[idx][:].reshape(1,3)\n",
    "                    \n",
    "            # output from the hidden layers is equal to sigmoid(x * W)\n",
    "            # we have already made sure the dimensions are compatible with one another \n",
    "            z = sigmoid(np.matmul(x , weights_hidden_input))\n",
    "                \n",
    "            # we insert a 1 before the z to account for the bias from the next set of weights\n",
    "            z_with_1 = np.hstack((np.ones((1,1)) , z))\n",
    "                    \n",
    "            # the final output will be sigmoid(z * w)\n",
    "            y_hat = sigmoid(np.matmul(z_with_1 , weights_output_hidden))\n",
    "                    \n",
    "            # error = (y-y_hat)^2\n",
    "            testing_error += (y_test[idx] - y_hat) ** 2\n",
    "\n",
    "        # averaging the error\n",
    "        testing_error /= len(X_test)\n",
    "\n",
    "\n",
    "        #appending the errors to the list\n",
    "        training_error_list.append(training_error)\n",
    "        testing_error_list.append(testing_error)\n",
    "    \n",
    "    # we are done with all epochs\n",
    "    # we would resize our lists of errors to avoid 2D or 3D arrays\n",
    "    training_error_list = np.array(training_error_list).reshape((epochs,1))\n",
    "    testing_error_list = np.array(testing_error_list).reshape((epochs,1))\n",
    "    \n",
    "    # a list of indices from 1 to epochs representing the epoch number\n",
    "    indices = [i for i in range(1,epochs+1)]\n",
    "    \n",
    "    # printing the final set of weights\n",
    "    print(\"Final set of weights:\")\n",
    "    print(\"Weights between input layer and hidden layer : \\n\" , weights_hidden_input)\n",
    "    print(\"Weights between hidden layer and output layer: \\n\" , weights_output_hidden)\n",
    "\n",
    "    # plotting the training and testing errors\n",
    "    plt.plot(indices , training_error_list , 'r' , label = \"Training error\")\n",
    "    plt.plot(indices , testing_error_list , 'b' , label = \"Testing error\")\n",
    "    plt.title(\"Training and Testing errors for N = {} and operation = {}\".format(N , network_type))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.grid(True)\n",
    "    plt.show() \n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. XOR model for $N = 1000$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(1000 , \"XOR\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. XOR model for $N = 2500$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(2500 , \"XOR\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. XOR Model for $N = 5000$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(5000 , \"XOR\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. AND model for $N = 1000$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(1000 , \"AND\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. AND Model for $N = 2500$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(2500 , \"AND\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. AND model for $N = 5000$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(5000 , \"AND\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. OR Model for $N = 1000$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(1000 , \"OR\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. OR model for N = 2500"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(2500 , \"OR\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. OR model for $N = 5000$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "backpropagate_and_report(5000 , \"OR\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### General observations\n",
    "1. In general, as N increases, the training and testing errors tends to decrease.\n",
    "<br>\n",
    "<br>\n",
    "2. Due to the random initialisation of weights, sometimes we get the testing error to be greater than training error, and sometimes vice- versa. Thus, most of the observations have been made keeping in mind the majority of those runs.\n",
    "<br>\n",
    "<br>\n",
    "3. None of the models are underfitting or overfitting. In fact, in most of the models, the training and testing errors turn out to pretty similar. During some runs, the training error might marginally exceed the testing error, or vice- versa, but in general, they are close to each other, and hence, we can say the models are decently trained.\n",
    "<br>\n",
    "<br>\n",
    "<b><u>Note: </u></b> the above observations have been made after checking majority of the runs. One or two particular runs <b>may</b> contradict these observations. \n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "bec9f652a4082306f869e119250ed899f786ad91c86aea518642b2537134d841"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}